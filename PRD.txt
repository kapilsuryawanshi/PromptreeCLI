Qwen CLI prompt for Promptree application

- Goal
	- Provide a CLI that records LLM conversations as a tree. Each node stores prompt + response + metadata. Asking under a node passes only the chain of ancestors for context to the LLM so users can branch into multiple lines of research without bloating LLM context.
- Background
	- When you chat with an LLM, it creates a long single thread of chat conversation. With this behavior I find following issues.
		- After a while when you accumulated long conversation then context window is bloated and the effectiveness of the LLM response declines.
		- Comprehending this chat history is difficult if we just look at the long chain of chat.
		- At a chat conversation user may get several questions to follow-up, whereas once he asks one questions and follow-up, he loses the track of other follow-up questions and the corresponding parent conversation. This limits the deep researching ability using LLM.
	- To resolve these drawbacks following overall design of the conversation interface is being proposed.
		- User should be able to fork/branch out thread at any conversation node to go deeper into the branch, so as to create a tree like structure of the chat conversation.
		- When user follow-up in a particular branch of the conversation tree, then LLM should consider only the conversation history which lead to this branch of the follow-up instead of the entire chat history.
		- User should be able to delete a conversation branch.
		- User should be able to browse the conversation tree to review the knowledge generated.
- Your task is to build python CLI application to implement these conversation style using the following guidelines.
	- The application persistent database should be implemented as SQLite database.
		- The database file should be created in the user's home directory.
		- Schema for conversations table should hold the following information per conversation
			- ID - auto numbered unique primary key of the conversation
			- Subject - Subject line of the conversation
			- Model_Name - LLM model used to perform this conversation
			- User_Prompt - User prompt asked in this conversation, should be able to store long text asked by user.
			- LLM_Response - LLM response provided for the user prompt, should be able to store long response text generated by LLM.
			- PID - ID of the parent conversation from which this conversation is continued. The top level questions will not have PID.
			- User_Prompt_Timestamp - The date and time when the prompt was submitted to LLM
			- LLM_Response_Timestamp - The date and time when the answer was generated by LLM
			- etc.
		- This application will communicate with local Ollama model. The user will start the application by passing the LLM model name at the command line argument.
		- As the application is started, the application will show a command prompt waiting for accepting the commands from user.
			- The prompt will have the format "{model: LLM model name} > "
		- The application will support following commands at this prompt.
			- quit
				- This will quit the application.
			- rm <id>[,<id>,...]
				- Removes the conversations and its sub conversation trees recursively starting from the given conversation id. System will get confirmation before removing the conversations.
			- edit <id> -subject "<new subject>" -parent <None|id>
- edit <id> -link <id>[,<id>,...] (link to multiple conversations)
- edit <id> -link None (remove all links)
- edit <id> -unlink <id>[,<id>,...] (unlink from specific conversations)
				- Updates the subject for the conversation with <id> with the <new subject>
				- Updates the parent for the conversation to another conversation or set to None to make this conversation as root conversation
			- list
				- Lists the top level conversations from database in following format, if any.
					-
					  ```
					  - <subject> (id: <id>, created on:<timestamp>)
					  - <subject> (id: <id>, created on:<timestamp>)
					  - <subject> (id: <id>, created on:<timestamp>)
					  ...
					  ```
					- The list will be sorted by User_Prompt_Timestamp, such that the most recent conversation will appear at the top.
			- open <id>
				- shows the conversation with given conversation id. prints the node and a compact subtree using ascii tree characters similar to below format.
					-
					  ```
					  Subject:
					  	<subject> 
					  Prompt: 
					  	<prompt> (id: <id>, created on:<timestamp>)
					  Response: 
					  	<response> (created on:<timestamp>)
					  Sub prompts:
					  	├─ <subject> (id: <id>, created on:<timestamp>)
					      ├─ <subject> (id: <id>, created on:<timestamp>)
					      │	└─ <subject> (id: <id>, created on:<timestamp>)
					      │    	└─ <subject> (id: <id>, created on:<timestamp>)
					      ├─ <subject> (id: <id>, created on:<timestamp>)
					      │	└─ <subject> (id: <id>, created on:<timestamp>)
					      ...
					  ```
				- If any subsequent conversation is performed, then this conversation will be considered as parent.
			- close
				- Close current conversation context, reset to root
			- ask [@<id>] <prompt>
				- <id> is the conversation id to be considered as parent id, under which the subsequent follow-up prompt is being executed.
					- This is optional.
					- When not provided, this conversation will be created as sub conversation of the recent conversation happened in the immediately previous "ask" command.
						- If there is no immediate previous "ask" command, then this conversation will be considered as top level conversation.
					- When system determines that there exists a parent conversation, then the prompt will also be enriched with all the earlier conversations as context history to the LLM.
						- This context history will be prepared by combining all the conversations (prompt and their responses) recursively from this parent until its root parent conversation, following the child-parent chain recursively until the root conversation.
				- <prompt> is the entire user prompt text to be sent to the LLM along with the context history.
				- When the command is executed, following should happen
					- The parent conversation will be determined and then accordingly context history of earlier conversation will be prepared.
					- The conversation context history and user prompt will be sent to the LLM. The current date and time for user prompt submission will be noted.
					- The LLM would generate the response which will be streamed back to the user on console. The date and time for the response generation completion will be noted.
					- The LLM will also auto generate brief subject line for current conversation.
					- The conversation database will be updated with this new conversation entry.
					- The current conversation id for this conversation from database will be remembered as parent conversation id, in case user asks follow-up prompt immediately after this conversation.
					- After the command execution is finished a small message like below to be shown
						-
						  ```text
						  Saved conversation id: 127 — subject: "Why X?"
						  ```
			- export <id> <file>
				- exports the conversation tree starting at given id into markdown format into a file
			- summarize <id>
				- summarize the conversation at given id
			- help
				- Provides detailed help for every supported command.
		- The text displayed in the console will have the following colors.
			- Subject line will always be in yellow color
			- User prompt will always be in cyan color
			- LLM Response will always be in green color
			- All errors will be in red colors
Qwen CLI prompt for Promptree application

- Goal
	- Provide a CLI that records LLM conversations as a tree. Each node stores prompt + response + metadata. Asking under a node passes only the chain of ancestors for context to the LLM so users can branch into multiple lines of research without bloating LLM context.
- Background
	- When you chat with an LLM, it creates a long single thread of chat conversation. With this behavior I find following issues.
		- After a while when you accumulated long conversation then context window is bloated and the effectiveness of the LLM response declines.
		- Comprehending this chat history is difficult if we just look at the long chain of chat.
		- At a chat conversation user may get several questions to follow-up, whereas once he asks one questions and follow-up, he loses the track of other follow-up questions and the corresponding parent conversation. This limits the deep researching ability using LLM.
	- To resolve these drawbacks following overall design of the conversation interface is being proposed.
		- User should be able to fork/branch out thread at any conversation node to go deeper into the branch, so as to create a tree like structure of the chat conversation.
		- When user follow-up in a particular branch of the conversation tree, then LLM should consider only the conversation history which lead to this branch of the follow-up instead of the entire chat history.
		- User should be able to delete a conversation branch.
		- User should be able to browse the conversation tree to review the knowledge generated.
- Your task is to build python CLI application to implement these conversation style using the following guidelines.
	- The application persistent database should be implemented as SQLite database.
		- The database file should be created in the user's home directory.
		- Schema for conversations table should hold the following information per conversation
			- ID - auto numbered unique primary key of the conversation
			- Subject - Subject line of the conversation
			- Model_Name - LLM model used to perform this conversation
			- User_Prompt - User prompt asked in this conversation, should be able to store long text asked by user.
			- LLM_Response - LLM response provided for the user prompt, should be able to store long response text generated by LLM.
			- PID - ID of the parent conversation from which this conversation is continued. The top level questions will not have PID.
			- User_Prompt_Timestamp - The date and time when the prompt was submitted to LLM
			- LLM_Response_Timestamp - The date and time when the answer was generated by LLM
			- etc.
		- This application will communicate with local Ollama model. The user will start the application by passing the LLM model name at the command line argument.
		- As the application is started, the application will show a command prompt waiting for accepting the commands from user.
			- The prompt will have the format "{model: LLM model name} > "
		- The application will support following commands at this prompt.
			- quit
				- This will quit the application.
			- exit
				- This is an alias for quit command.
			- rm <id>[,<id>,...]
				- Removes the conversations and its sub conversation trees recursively starting from the given conversation id. System will get confirmation before removing the conversations.
			- edit <id> -subject "<new subject>" -parent <None|id>
- edit <id> -link <id>[,<id>,...] (link to multiple conversations)
- edit <id> -link None (remove all links)
- edit <id> -unlink <id>[,<id>,...] (unlink from specific conversations)
- edit <id> (opens conversation in external editor in plain text format for comprehensive editing)
				- Updates the subject for the conversation with <id> with the <new subject>
				- Updates the parent for the conversation to another conversation or set to None to make this conversation as root conversation
				- The comprehensive editing feature opens a temporary text file with the conversation details in plain text format, allowing users to edit subject, parent ID, user prompt, LLM response, and linked conversations. The file format includes markers like USER_PROMPT_START/USER_PROMPT_END and LLM_RESPONSE_START/LLM_RESPONSE_END for editing the prompt and response content.
			- list
				- Lists the top level conversations from database in following format, if any.
					-
					  ```
					  - <subject> (id: <id>, created on:<timestamp>)
					  - <subject> (id: <id>, created on:<timestamp>)
					  - <subject> (id: <id>, created on:<timestamp>)
					  ...
					  ```
					- The list will be sorted by User_Prompt_Timestamp, such that the most recent conversation will appear at the top.
			- open <id>
				- shows the conversation with given conversation id. prints the node and a compact subtree using ascii tree characters similar to below format.
					-
					  ```
					  Subject:
					  	<subject> 
					  Prompt: 
					  	<prompt> (id: <id>, created on:<timestamp>)
					  Response: 
					  	<response> (created on:<timestamp>)
					  Sub prompts:
					  	├─ <subject> (id: <id>, created on:<timestamp>)
					      ├─ <subject> (id: <id>, created on:<timestamp>)
					      │	└─ <subject> (id: <id>, created on:<timestamp>)
					      │    	└─ <subject> (id: <id>, created on:<timestamp>)
					      ├─ <subject> (id: <id>, created on:<timestamp>)
					      │	└─ <subject> (id: <id>, created on:<timestamp>)
					      ...
					  ```
				- If any subsequent conversation is performed, then this conversation will be considered as parent.
			- close
				- Close current conversation context, reset to root
			- ask [@<id>] <prompt>
				- <id> is the conversation id to be considered as parent id, under which the subsequent follow-up prompt is being executed.
					- This is optional.
					- When not provided, this conversation will be created as sub conversation of the recent conversation happened in the immediately previous "ask" command.
						- If there is no immediate previous "ask" command, then this conversation will be considered as top level conversation.
					- When system determines that there exists a parent conversation, then the prompt will also be enriched with all the earlier conversations as context history to the LLM.
						- This context history will be prepared by combining all the conversations (prompt and their responses) recursively from this parent until its root parent conversation, following the child-parent chain recursively until the root conversation.
				- <prompt> is the entire user prompt text to be sent to the LLM along with the context history.
				- When the command is executed, following should happen
					- The parent conversation will be determined and then accordingly context history of earlier conversation will be prepared.
					- The conversation context history and user prompt will be sent to the LLM. The current date and time for user prompt submission will be noted.
					- The LLM would generate the response which will be streamed back to the user on console. The date and time for the response generation completion will be noted.
					- The LLM will also auto generate brief subject line for current conversation.
					- The conversation database will be updated with this new conversation entry.
					- The current conversation id for this conversation from database will be remembered as parent conversation id, in case user asks follow-up prompt immediately after this conversation.
					- After the command execution is finished a small message like below to be shown
						-
						  ```text
						  Saved conversation id: 127 — subject: "Why X?"
						  ```
			- ask
				- Opens an external editor to input a longer prompt and parent ID from a file (when no arguments provided)
				- When 'ask' is used without arguments, it opens a temporary text file with PARENT_ID and USER_PROMPT sections for users to input longer prompts and specify the parent conversation ID. The file format includes markers like USER_PROMPT_START/USER_PROMPT_END for editing the prompt content.
			- add
				- Opens an external editor to manually add a conversation with parent, links, prompt and response
				- When 'add' is used, it opens a temporary text file that includes fields for PARENT_ID, LINKED_CONVERSATIONS_ID, USER_PROMPT, and LLM_RESPONSE. The file format includes markers like USER_PROMPT_START/USER_PROMPT_END and LLM_RESPONSE_START/LLM_RESPONSE_END for editing the prompt and response content. The LLM generates a subject line based on the prompt and response, then adds the conversation to the database with the specified parent and links.
			- export <id> <file>
				- exports the conversation tree starting at given id into markdown format into a file
			- summarize <id>
				- summarize the conversation at given id
			- search <text>
				- Search for text in conversations (supports wildcard characters * and case-insensitive matching)
				- Lists subjects of all matching conversations
			- help
				- Provides detailed help for every supported command.
		- The application uses external editors for file-based editing in edit, ask, and add commands:
			- The application first checks for the EDITOR environment variable
			- If not set, it falls back to the VISUAL environment variable
			- On Windows, if neither is set, it defaults to notepad
			- On Unix-like systems, it defaults to nano
			- Users can configure their preferred editor via environment variables:
				- For VS Code: set EDITOR=code --wait
				- For Sublime Text: set EDITOR=subl -w
				- For default system application on Windows: set EDITOR=default
				- For Notepad++: set EDITOR=notepad++ -multiInst -notabbar -nosession -noPlugin
			- The text displayed in the console will have the following colors.
			- Subject line will always be in yellow color
			- User prompt will always be in cyan color
			- LLM Response will always be in green color
			- All errors will be in red colors